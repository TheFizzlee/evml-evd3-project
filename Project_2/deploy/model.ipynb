{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "TRAIN_DIR = '../prep/preprocessed_images/train'\n",
    "TEST_DIR = '../prep/preprocessed_images/test'\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "STEP_SIZE = 5  \n",
    "GAMMA = 0.1\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define k fold cross validation\n",
    "K_FOLDS = 5\n",
    "\n",
    "metrics = {\n",
    "    'validation loss': [],\n",
    "    'accuracy': [],\n",
    "    'precision': [],\n",
    "    'recall': [],\n",
    "    'f1': []\n",
    "}\n",
    "\n",
    "best_metrics = {\n",
    "    'validation loss': [0] * K_FOLDS,\n",
    "    'accuracy': [0] * K_FOLDS,\n",
    "    'precision': [0] * K_FOLDS,\n",
    "    'recall': [0] * K_FOLDS,\n",
    "    'f1': [0] * K_FOLDS\n",
    "}\n",
    "\n",
    "kf = StratifiedKFold(n_splits=K_FOLDS, random_state=RANDOM_STATE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device: use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2\n",
    "model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the convolutional layers (feature extraction part)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the classifier part to suit the task (3 classes)\n",
    "model.classifier[1] = nn.Linear(model.classifier[1].in_features, 3)  # 3 classes: rock, paper, scissors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MobileNetV2(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "    (1): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (2): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
       "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
       "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (6): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
       "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (9): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (10): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (11): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
       "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (12): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (13): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
       "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (15): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (16): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (17): InvertedResidual(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU6(inplace=True)\n",
       "        )\n",
       "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (18): Conv2dNormActivation(\n",
       "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU6(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "    (1): Linear(in_features=1280, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move the model to the GPU (if available)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Learning rate scheduler\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping parameters\n",
    "patience = 3\n",
    "best_val_loss = float('inf')\n",
    "best_model_wts = None\n",
    "\n",
    "best_overall_val_loss = float('inf')\n",
    "best_model_wts_overall = None\n",
    "best_overal_epoch_for_fold = -1\n",
    "best_fold = -1\n",
    "\n",
    "patience_counter = 0\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transformation for the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset\n",
    "full_training_dataset = ImageFolder(root=TRAIN_DIR, transform=transform)  # No need for separate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only the labels from the dataset\n",
    "targets = [full_training_dataset.targets[i] for i in range(len(full_training_dataset))]\n",
    "\n",
    "# Define the stratified split\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n",
    "\n",
    "# Obtain train and validation indices\n",
    "for train_idx, val_idx in stratified_split.split(np.zeros(len(targets)), targets):\n",
    "    train_set = Subset(full_training_dataset, train_idx)\n",
    "    val_set = Subset(full_training_dataset, val_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 414\n",
      "Validation set size: 104\n",
      "Test set size: 133\n"
     ]
    }
   ],
   "source": [
    "# Load all the data\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(ImageFolder(root=TEST_DIR, transform=transform), batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print('Training set size:', len(train_set))\n",
    "print('Validation set size:', len(val_set))\n",
    "print('Test set size:', len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1/5\n",
      "Epoch 1\n",
      "Loss: 2.2059, Accuracy: 64.65%\n",
      "Validation Loss: 1.4543, Accuracy: 72.29%\n",
      "Precision: 0.9320, Recall: 0.7229, F1: 0.7716\n",
      "Epoch 2\n",
      "Loss: 0.0815, Accuracy: 96.37%\n",
      "Validation Loss: 0.1679, Accuracy: 96.39%\n",
      "Precision: 0.9677, Recall: 0.9639, F1: 0.9644\n",
      "Epoch 3\n",
      "Loss: 0.0169, Accuracy: 99.09%\n",
      "Validation Loss: 0.0771, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9879\n",
      "Epoch 4\n",
      "Loss: 0.0054, Accuracy: 100.00%\n",
      "Validation Loss: 0.0187, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9879\n",
      "Epoch 5\n",
      "Loss: 0.0080, Accuracy: 99.40%\n",
      "Validation Loss: 0.0003, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 6\n",
      "Loss: 0.0127, Accuracy: 99.40%\n",
      "Validation Loss: 0.0002, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 7\n",
      "Loss: 0.0011, Accuracy: 100.00%\n",
      "Validation Loss: 0.0004, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 8\n",
      "Loss: 0.0005, Accuracy: 100.00%\n",
      "Validation Loss: 0.0003, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 9\n",
      "Loss: 0.0024, Accuracy: 100.00%\n",
      "Validation Loss: 0.0004, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 9\n",
      "\n",
      "New best model from fold 1 with validation loss: 0.0002\n",
      "\n",
      "Restored model from epoch 6 with validation loss: 0.0002\n",
      "\n",
      "Fold 1 completed.\n",
      "\n",
      "Fold 2/5\n",
      "Epoch 1\n",
      "Loss: 2.0576, Accuracy: 69.18%\n",
      "Validation Loss: 0.2197, Accuracy: 96.39%\n",
      "Precision: 0.9777, Recall: 0.9639, F1: 0.9697\n",
      "Epoch 2\n",
      "Loss: 0.0981, Accuracy: 96.37%\n",
      "Validation Loss: 0.0768, Accuracy: 96.39%\n",
      "Precision: 0.9647, Recall: 0.9639, F1: 0.9637\n",
      "Epoch 3\n",
      "Loss: 0.0370, Accuracy: 99.09%\n",
      "Validation Loss: 0.0257, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9879\n",
      "Epoch 4\n",
      "Loss: 0.0593, Accuracy: 98.79%\n",
      "Validation Loss: 0.0002, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 5\n",
      "Loss: 0.0361, Accuracy: 99.09%\n",
      "Validation Loss: 0.1471, Accuracy: 96.39%\n",
      "Precision: 0.9651, Recall: 0.9639, F1: 0.9638\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 6\n",
      "Loss: 0.0333, Accuracy: 98.49%\n",
      "Validation Loss: 0.0408, Accuracy: 97.59%\n",
      "Precision: 0.9763, Recall: 0.9759, F1: 0.9759\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 7\n",
      "Loss: 0.0064, Accuracy: 99.70%\n",
      "Validation Loss: 0.0301, Accuracy: 98.80%\n",
      "Precision: 0.9883, Recall: 0.9880, F1: 0.9879\n",
      "No improvement detected. Patience counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 7\n",
      "\n",
      "New best model from fold 2 with validation loss: 0.0002\n",
      "\n",
      "Restored model from epoch 4 with validation loss: 0.0002\n",
      "\n",
      "Fold 2 completed.\n",
      "\n",
      "Fold 3/5\n",
      "Epoch 1\n",
      "Loss: 2.1459, Accuracy: 66.16%\n",
      "Validation Loss: 0.2563, Accuracy: 91.57%\n",
      "Precision: 0.9341, Recall: 0.9157, F1: 0.9179\n",
      "Epoch 2\n",
      "Loss: 0.0714, Accuracy: 97.58%\n",
      "Validation Loss: 0.0004, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 3\n",
      "Loss: 0.1776, Accuracy: 96.68%\n",
      "Validation Loss: 0.0182, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9880\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 4\n",
      "Loss: 0.0806, Accuracy: 97.89%\n",
      "Validation Loss: 0.0919, Accuracy: 95.18%\n",
      "Precision: 0.9585, Recall: 0.9518, F1: 0.9515\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 5\n",
      "Loss: 0.0151, Accuracy: 99.70%\n",
      "Validation Loss: 0.0098, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 5\n",
      "\n",
      "Restored model from epoch 2 with validation loss: 0.0004\n",
      "\n",
      "Fold 3 completed.\n",
      "\n",
      "Fold 4/5\n",
      "Epoch 1\n",
      "Loss: 2.1490, Accuracy: 66.47%\n",
      "Validation Loss: 0.2339, Accuracy: 92.77%\n",
      "Precision: 0.9331, Recall: 0.9277, F1: 0.9279\n",
      "Epoch 2\n",
      "Loss: 0.0153, Accuracy: 99.09%\n",
      "Validation Loss: 0.1231, Accuracy: 96.39%\n",
      "Precision: 0.9679, Recall: 0.9639, F1: 0.9641\n",
      "Epoch 3\n",
      "Loss: 0.0634, Accuracy: 98.19%\n",
      "Validation Loss: 0.0150, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9879\n",
      "Epoch 4\n",
      "Loss: 0.0693, Accuracy: 97.28%\n",
      "Validation Loss: 0.1664, Accuracy: 95.18%\n",
      "Precision: 0.9570, Recall: 0.9518, F1: 0.9519\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 5\n",
      "Loss: 0.0328, Accuracy: 98.49%\n",
      "Validation Loss: 0.0309, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9880\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 6\n",
      "Loss: 0.0223, Accuracy: 99.40%\n",
      "Validation Loss: 0.0309, Accuracy: 98.80%\n",
      "Precision: 0.9884, Recall: 0.9880, F1: 0.9880\n",
      "No improvement detected. Patience counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 6\n",
      "\n",
      "Restored model from epoch 3 with validation loss: 0.0150\n",
      "\n",
      "Fold 4 completed.\n",
      "\n",
      "Fold 5/5\n",
      "Epoch 1\n",
      "Loss: 2.0901, Accuracy: 67.17%\n",
      "Validation Loss: 0.6699, Accuracy: 87.80%\n",
      "Precision: 0.9333, Recall: 0.8780, F1: 0.8957\n",
      "Epoch 2\n",
      "Loss: 0.0727, Accuracy: 97.29%\n",
      "Validation Loss: 0.0048, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 3\n",
      "Loss: 0.0399, Accuracy: 97.59%\n",
      "Validation Loss: 0.0222, Accuracy: 97.56%\n",
      "Precision: 0.9774, Recall: 0.9756, F1: 0.9757\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 4\n",
      "Loss: 0.0937, Accuracy: 96.99%\n",
      "Validation Loss: 0.0048, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 5\n",
      "Loss: 0.0319, Accuracy: 99.10%\n",
      "Validation Loss: 0.0507, Accuracy: 98.78%\n",
      "Precision: 0.9883, Recall: 0.9878, F1: 0.9878\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 6\n",
      "Loss: 0.0211, Accuracy: 99.10%\n",
      "Validation Loss: 0.0055, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 7\n",
      "Loss: 0.0036, Accuracy: 100.00%\n",
      "Validation Loss: 0.0010, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 8\n",
      "Loss: 0.0061, Accuracy: 99.70%\n",
      "Validation Loss: 0.0006, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "Epoch 9\n",
      "Loss: 0.0053, Accuracy: 100.00%\n",
      "Validation Loss: 0.0010, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 1/3\n",
      "Epoch 10\n",
      "Loss: 0.0006, Accuracy: 100.00%\n",
      "Validation Loss: 0.0009, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 2/3\n",
      "Epoch 11\n",
      "Loss: 0.0009, Accuracy: 100.00%\n",
      "Validation Loss: 0.0008, Accuracy: 100.00%\n",
      "Precision: 1.0000, Recall: 1.0000, F1: 1.0000\n",
      "No improvement detected. Patience counter: 3/3\n",
      "\n",
      "Early stopping triggered at epoch 11\n",
      "\n",
      "Restored model from epoch 8 with validation loss: 0.0006\n",
      "\n",
      "Fold 5 completed.\n",
      "\n",
      "Best model from fold 2 with validation loss: 0.0002 loaded.\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_set, [train_set[i][1] for i in range(len(train_set))])):\n",
    "    print(f\"\\nFold {fold + 1}/{K_FOLDS}\")\n",
    "\n",
    "    patience_counter = 0 # Reset patience counter for each fold\n",
    "    epoch_counter = 0 # Reset epoch counter for each fold\n",
    "\n",
    "    best_val_loss = float('inf') # Reset best validation loss for each fold\n",
    "    best_model_wts = None # Reset best model weights for each fold\n",
    "\n",
    "    # Prepare data for this fold\n",
    "    train_subset = Subset(train_set, train_idx)\n",
    "    val_subset = Subset(train_set, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Reset the model and optimizer for each fold\n",
    "    model = models.mobilenet_v2(weights=MobileNet_V2_Weights.DEFAULT)\n",
    "    model.to(device)\n",
    "    optimizer = Adam(model.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    while True:\n",
    "        model.train()  # Set the model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for images, label in train_loader:\n",
    "            images, labels = images.to(device), label.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero the gradients\n",
    "            output = model(images)\n",
    "            loss = criterion(output, label)\n",
    "            loss.backward()  # Backpropagation\n",
    "            optimizer.step()  # Update the weights\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            _, predicted = output.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        # Update the scheduler after the optimizer step\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss = running_loss / len(train_set)\n",
    "        train_accuracy = 100. * correct / total\n",
    "        print(f'Epoch {epoch_counter + 1}')\n",
    "        print(f'Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()  # Set the model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                output = model(images)\n",
    "                loss = criterion(output, labels)\n",
    "\n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = output.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss /= len(val_set)\n",
    "        val_accuracy = 100. * correct / total\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "        # Calculate metrics: precision, recall, f1 score\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=1)\n",
    "\n",
    "        print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')\n",
    "\n",
    "        # Store the metrics\n",
    "        metrics['accuracy'].append(val_accuracy)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_wts = model.state_dict()\n",
    "            best_epoch = epoch_counter\n",
    "            patience_counter = 0\n",
    "\n",
    "            # store the metrics and overwrite the old metrics\n",
    "            best_metrics['accuracy'][fold] = val_accuracy\n",
    "            best_metrics['precision'][fold] = precision\n",
    "            best_metrics['recall'][fold] = recall\n",
    "            best_metrics['f1'][fold] = f1\n",
    "        else:\n",
    "            patience_counter += 1  # Increase the patience counter\n",
    "            print(f'No improvement detected. Patience counter: {patience_counter}/{patience}')\n",
    "\n",
    "        # Check if patience has run out\n",
    "        if patience_counter >= patience:\n",
    "            print(f'\\nEarly stopping triggered at epoch {epoch_counter + 1}')\n",
    "            break # Stop the training loop\n",
    "\n",
    "        epoch_counter += 1\n",
    "    \n",
    "    # Store the best model from this fold if its validation loss is the best so far\n",
    "    if best_val_loss < best_overall_val_loss:\n",
    "        print(f\"\\nNew best model from fold {fold + 1} with validation loss: {best_val_loss:.4f}\")\n",
    "\n",
    "        best_overall_val_loss = best_val_loss\n",
    "        best_model_wts_overall = best_model_wts\n",
    "        best_fold = fold\n",
    "        best_overal_epoch_for_fold = best_epoch\n",
    "\n",
    "    # Load the best model weights\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(f'\\nRestored model from epoch {best_epoch + 1} with validation loss: {best_val_loss:.4f}')\n",
    "    \n",
    "    print(f\"\\nFold {fold + 1} completed.\")\n",
    "\n",
    "# Load the best model weights from all the folds\n",
    "if best_model_wts_overall is not None:\n",
    "    model.load_state_dict(best_model_wts_overall)\n",
    "    print(f\"\\nBest model from fold {best_fold + 1} with validation loss: {best_overall_val_loss:.4f} loaded.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 100.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "All Metrics:\n",
      "\n",
      "Fold 1 Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 100.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Fold 2 Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 100.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Fold 3 Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 100.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n",
      "\n",
      "Fold 4 Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 98.7952\n",
      "Precision: 0.9884\n",
      "Recall: 0.9880\n",
      "F1 Score: 0.9879\n",
      "\n",
      "Fold 5 Metrics:\n",
      "Validation Loss: 0.0000\n",
      "Accuracy: 100.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Print best model metrics\n",
    "print(\"\\nBest Model Metrics:\")\n",
    "print(f\"Validation Loss: {best_metrics['validation loss'][best_fold]:.4f}\")\n",
    "print(f\"Accuracy: {best_metrics['accuracy'][best_fold]:.4f}\")\n",
    "print(f\"Precision: {best_metrics['precision'][best_fold]:.4f}\")\n",
    "print(f\"Recall: {best_metrics['recall'][best_fold]:.4f}\")\n",
    "print(f\"F1 Score: {best_metrics['f1'][best_fold]:.4f}\")\n",
    "\n",
    "# Print all metrics for all folds\n",
    "print(\"\\nAll Metrics:\")\n",
    "for i in range(K_FOLDS):\n",
    "    print(f\"\\nFold {i + 1} Metrics:\")\n",
    "    print(f\"Validation Loss: {best_metrics['validation loss'][i]:.4f}\")\n",
    "    print(f\"Accuracy: {best_metrics['accuracy'][i]:.4f}\")\n",
    "    print(f\"Precision: {best_metrics['precision'][i]:.4f}\")\n",
    "    print(f\"Recall: {best_metrics['recall'][i]:.4f}\")\n",
    "    print(f\"F1 Score: {best_metrics['f1'][i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0008, Test Accuracy: 100.00%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        40\n",
      "           1       1.00      1.00      1.00        54\n",
      "           2       1.00      1.00      1.00        39\n",
      "\n",
      "    accuracy                           1.00       133\n",
      "   macro avg       1.00      1.00      1.00       133\n",
      "weighted avg       1.00      1.00      1.00       133\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  0  0]\n",
      " [ 0 54  0]\n",
      " [ 0  0 39]]\n"
     ]
    }
   ],
   "source": [
    "# Test loop\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = 100. * correct / total\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "# Classification report\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(all_labels, all_preds))\n",
    "\n",
    "# Confusion matrix\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(all_labels, all_preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model, './model/model.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
