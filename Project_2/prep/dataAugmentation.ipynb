{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100  # Size of the noise vector\n",
    "num_classes = 3  # Number of labels (rock, paper, scissors)\n",
    "image_size = 170  # Image resolution\n",
    "batch_size = 170  # Batch size\n",
    "num_epochs = 200  # Number of training epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Use GPU if available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(n_epochs=50, batch_size=16, lr=0.0002, b1=0.5, b2=0.999, latent_dim=100, img_size_x=256, img_size_y=341, channels=3, n_classes=10, display_port=8097, display_server='http://localhost', sample_interval=5)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n_epochs', type=int, default=50, help='number of epochs of training')\n",
    "parser.add_argument('--batch_size', type=int, default=16, help='size of the batches')\n",
    "parser.add_argument('--lr', type=float, default=0.0002, help='adam: learning rate')\n",
    "parser.add_argument('--b1', type=float, default=0.5, help='adam: beta 1')\n",
    "parser.add_argument('--b2', type=float, default=0.999, help='adam: beta 2')\n",
    "parser.add_argument('--latent_dim', type=int, default=100, help='dimensionality of the latent space')\n",
    "parser.add_argument('--img_size_x', type=int, default=256, help='size of each image dimension')\n",
    "parser.add_argument('--img_size_y', type=int, default=341, help='size of each image dimension')\n",
    "parser.add_argument('--channels', type=int, default=3, help='number of image channels')\n",
    "parser.add_argument('--n_classes', type=int, default=10, help='number of classes (e.g., digits 0 ..9, 10 classes on mnist)')\n",
    "parser.add_argument('--display_port', type=int, default=8097, help='where to run the visdom for visualization? useful if running multiple visdom tabs')\n",
    "parser.add_argument('--display_server', type=str, default=\"http://localhost\", help='visdom server of the web display')\n",
    "parser.add_argument('--sample_interval', type=int, default=5, help='interval between image samples')\n",
    "\n",
    "# Prevent argparse from interpreting Jupyter arguments\n",
    "opt = parser.parse_args(args=[])\n",
    "\n",
    "# Print the arguments for verification\n",
    "print(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import visdom\n",
    "    vis = visdom.Visdom(server=opt.display_server, port=opt.display_port, raise_exceptions=True) # Create vis env.\n",
    "except ImportError:\n",
    "    vis = None\n",
    "else:\n",
    "    assert vis.check_connection(timeout_seconds=3), \"No connection could be formed quickly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dims = (opt.channels, opt.img_size_x, opt.img_size_y)\n",
    "n_features = opt.channels * opt.img_size_x * opt.img_size_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        nn.init.xavier_normal_(m.weight.data, gain=0.02)\n",
    "    elif classname.find('BatchNorm1d') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # Map z & y (noise and label) into the hidden layer.\n",
    "        # TO DO: How to run this with a function defined here?\n",
    "        self.z_map = nn.Sequential(\n",
    "            nn.Linear(opt.latent_dim, 200),\n",
    "            nn.BatchNorm1d(200),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.y_map = nn.Sequential(\n",
    "            nn.Linear(opt.n_classes, 1000),\n",
    "            nn.BatchNorm1d(1000),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.zy_map = nn.Sequential(\n",
    "            nn.Linear(1200, 1200),\n",
    "            nn.BatchNorm1d(1200),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(1200, n_features),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # Tanh > Image values are between [-1, 1]\n",
    "\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        zh = self.z_map(z)\n",
    "        yh = self.y_map(y)\n",
    "        zy = torch.cat((zh, yh), dim=1) # Combine noise and labels.\n",
    "        zyh = self.zy_map(zy)\n",
    "        x = self.model(zyh)\n",
    "        x = x.view(x.size(0), *img_dims)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(240, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Imitating a 3d array by combining second and third dimensions via multiplication for maxout.\n",
    "        self.x_map = nn.Sequential(nn.Linear(n_features, 240 * 5))\n",
    "        self.y_map = nn.Sequential(nn.Linear(opt.n_classes, 50 * 5))\n",
    "        self.j_map = nn.Sequential(nn.Linear(240 + 50, 240 * 4))\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # maxout for x\n",
    "        print(x.shape)\n",
    "        x = x.view(-1, n_features)\n",
    "        x = self.x_map(x)\n",
    "        x, _ = x.view(-1, 240, 5).max(dim=2) # pytorch outputs max values and indices\n",
    "        # .. and y\n",
    "        y = y.view(-1, opt.n_classes)\n",
    "        y = self.y_map(y)\n",
    "        y, _ = y.view(-1, 50, 5).max(dim=2)\n",
    "        # joint maxout layer\n",
    "        jmx = torch.cat((x, y), dim=1)\n",
    "        jmx = self.j_map(jmx)\n",
    "        jmx, _ = jmx.view(-1, 240, 4).max(dim=2)\n",
    "\n",
    "        prob = self.model(jmx)\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Paper', 'Rock', 'Scissor']\n",
      "133\n"
     ]
    }
   ],
   "source": [
    "# Loading local dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((opt.img_size_x,opt.img_size_y)),  # Resize to the required image size\n",
    "    transforms.ToTensor(),  # Convert image to tensor\n",
    "    transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))  # Normalize image to [-1, 1]\n",
    "])\n",
    "\n",
    "image_dir = './images'  # Path to your image dataset\n",
    "\n",
    "# Load images using ImageFolder\n",
    "dataset = datasets.ImageFolder(root=image_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=opt.batch_size, shuffle=True)\n",
    "\n",
    "# Check image labels\n",
    "print(dataset.classes)\n",
    "\n",
    "# Check amount of images loaded\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=240, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (x_map): Sequential(\n",
       "    (0): Linear(in_features=261888, out_features=1200, bias=True)\n",
       "  )\n",
       "  (y_map): Sequential(\n",
       "    (0): Linear(in_features=10, out_features=250, bias=True)\n",
       "  )\n",
       "  (j_map): Sequential(\n",
       "    (0): Linear(in_features=290, out_features=960, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "gan_loss = nn.BCELoss()\n",
    "\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "\n",
    "# Loss record.\n",
    "g_losses = []\n",
    "d_losses = []\n",
    "epochs = []\n",
    "loss_legend = ['Discriminator', 'Generator']\n",
    "\n",
    "if cuda:\n",
    "    generator = generator.cuda()\n",
    "    discriminator = discriminator.cuda()\n",
    "\n",
    "# Weight initialization.\n",
    "generator.apply(weights_init_xavier)\n",
    "discriminator.apply(weights_init_xavier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "Epoch: 0, Batch: 0, Batches Done: 0\n",
      "Epoch 0, Batch 0, Batches Done: 0\n",
      "D Loss: 1.3863874673843384, G Loss: 0.6840237975120544\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "Epoch: 0, Batch: 1, Batches Done: 1\n",
      "Epoch 0, Batch 1, Batches Done: 1\n",
      "D Loss: 1.355218529701233, G Loss: 0.6666851043701172\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "torch.Size([16, 3, 256, 341])\n",
      "Epoch: 0, Batch: 2, Batches Done: 2\n",
      "Epoch 0, Batch 2, Batches Done: 2\n",
      "D Loss: 1.283057451248169, G Loss: 0.5031737089157104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[193], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(opt\u001b[38;5;241m.\u001b[39mn_epochs):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch))\n\u001b[1;32m----> 3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Labels for real and fake images\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfake\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVariable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequires_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torchvision\\datasets\\folder.py:247\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader(path)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    249\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torchvision\\transforms\\functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Timoy\\anaconda3\\envs\\minor\\Lib\\site-packages\\PIL\\Image.py:2365\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2353\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2354\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2355\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[0;32m   2356\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[0;32m   2357\u001b[0m         )\n\u001b[0;32m   2358\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2359\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2360\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2361\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[0;32m   2362\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[0;32m   2363\u001b[0m         )\n\u001b[1;32m-> 2365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.n_epochs):\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    for i, (batch, labels) in enumerate(dataloader):\n",
    "\n",
    "        # Labels for real and fake images\n",
    "        real = Variable(Tensor(batch.size(0), 1).fill_(1), requires_grad=False)\n",
    "        fake = Variable(Tensor(batch.size(0), 1).fill_(0), requires_grad=False)\n",
    "\n",
    "        # One-hot encode labels\n",
    "        labels_onehot = Variable(Tensor(batch.size(0), opt.n_classes).zero_())\n",
    "        labels_ = labels.type(LongTensor)  # Ensure labels are LongTensor\n",
    "        labels_ = labels_.view(batch.size(0), 1)\n",
    "        labels_onehot = labels_onehot.scatter_(1, labels_, 1)\n",
    "\n",
    "        # Real and fake images\n",
    "        imgs_real = Variable(batch.type(Tensor))\n",
    "        noise = Variable(Tensor(batch.size(0), opt.latent_dim).normal_(0, 1))\n",
    "        imgs_fake = generator(noise, labels_onehot)\n",
    "\n",
    "        # == Discriminator update == #\n",
    "        optimizer_D.zero_grad()\n",
    "        d_loss = gan_loss(discriminator(imgs_real, labels_onehot), real) + \\\n",
    "                 gan_loss(discriminator(imgs_fake, labels_onehot), fake)\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # == Generator update == #\n",
    "        noise = Variable(Tensor(batch.size(0), opt.latent_dim).normal_(0, 1))\n",
    "        imgs_fake = generator(noise, labels_onehot)\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = gan_loss(discriminator(imgs_fake, labels_onehot), real)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # == Visdom updates == #\n",
    "        if vis:\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            print(f\"Epoch: {epoch}, Batch: {i}, Batches Done: {batches_done}\")\n",
    "            if batches_done % opt.sample_interval >= 0:\n",
    "\n",
    "                # Append losses for plotting\n",
    "                epochs.append(batches_done)\n",
    "                g_losses.append(g_loss.item())\n",
    "                d_losses.append(d_loss.item())\n",
    "\n",
    "                # Update loss plot\n",
    "                vis.line(\n",
    "                    X=torch.tensor([batches_done]),\n",
    "                    Y=torch.tensor([[d_loss.item(), g_loss.item()]]),\n",
    "                    win=1,\n",
    "                    update='append' if batches_done > 0 else None,\n",
    "                    opts={\n",
    "                        'title': 'Loss over time',\n",
    "                        'legend': ['D Loss', 'G Loss'],\n",
    "                        'xlabel': 'Batches Done',\n",
    "                        'ylabel': 'Loss',\n",
    "                        'width': 512,\n",
    "                        'height': 512,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # Update generated images\n",
    "                noise = Variable(Tensor(5 * 10, opt.latent_dim).normal_(0, 1))\n",
    "                labels_onehot = Variable(Tensor(5 * 10, opt.n_classes).zero_())\n",
    "                labels_ = torch.arange(0, 10).repeat(5, 1).transpose(0, 1).contiguous().view(-1, 1)\n",
    "                labels_ = labels_.type(LongTensor)\n",
    "                labels_onehot = labels_onehot.scatter_(1, labels_, 1)\n",
    "                imgs_fake = generator(noise, labels_onehot)\n",
    "\n",
    "                vis.images(\n",
    "                    imgs_fake.data[:50],\n",
    "                    nrow=5,\n",
    "                    win=2,\n",
    "                    opts={\n",
    "                        'title': 'Generated Images [Epoch {}, Batch {}]'.format(epoch, i),\n",
    "                        'width': 512,\n",
    "                        'height': 512,\n",
    "                    }\n",
    "                )\n",
    "                print(f\"Epoch {epoch}, Batch {i}, Batches Done: {batches_done}\")\n",
    "                print(f\"D Loss: {d_loss.item()}, G Loss: {g_loss.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 341])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Timoy\\AppData\\Local\\Temp\\ipykernel_18132\\2758790539.py:6: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  labels_ = torch.range(0, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0 min: -0.1117442175745964, max: 0.4447222650051117\n",
      "Image 1 min: -0.06747356057167053, max: 0.3155316710472107\n",
      "Image 2 min: -0.768531084060669, max: 0.9701910614967346\n",
      "Image 3 min: -0.0633787140250206, max: 0.26418811082839966\n",
      "Image 4 min: -0.518787682056427, max: 0.8570298552513123\n",
      "Image 5 min: -0.33012157678604126, max: 0.7592254877090454\n",
      "Image 6 min: -0.124612957239151, max: 0.4686332643032074\n",
      "Image 7 min: -0.8240763545036316, max: 0.9849047660827637\n",
      "Image 8 min: -0.09766220301389694, max: 0.3914071023464203\n",
      "Image 9 min: -0.6644922494888306, max: 0.9387771487236023\n",
      "Image 10 min: -1.0, max: 1.0\n",
      "Image 11 min: -1.0, max: 1.0\n",
      "Image 12 min: -1.0, max: 1.0\n",
      "Image 13 min: -0.9999998211860657, max: 1.0\n",
      "Image 14 min: -1.0, max: 1.0\n",
      "Image 15 min: -0.7621409893035889, max: 0.953784167766571\n",
      "Image 16 min: -0.6685002446174622, max: 0.9178050756454468\n",
      "Image 17 min: -0.36280301213264465, max: 0.6913214325904846\n",
      "Image 18 min: -0.6035997867584229, max: 0.8606859445571899\n",
      "Image 19 min: -0.6618451476097107, max: 0.8999820947647095\n",
      "Image 20 min: -0.9019632339477539, max: 0.9927876591682434\n",
      "Image 21 min: -0.06473805010318756, max: 0.27743321657180786\n",
      "Image 22 min: -0.05730549991130829, max: 0.22488224506378174\n",
      "Image 23 min: -0.05872371792793274, max: 0.23979152739048004\n",
      "Image 24 min: -0.217544823884964, max: 0.5232125520706177\n",
      "Image 25 min: -0.05742855370044708, max: 0.22372063994407654\n",
      "Image 26 min: -0.6650779843330383, max: 0.9096214175224304\n",
      "Image 27 min: -0.8325302600860596, max: 0.9799470901489258\n",
      "Image 28 min: -0.15022219717502594, max: 0.4419272541999817\n",
      "Image 29 min: -0.13374865055084229, max: 0.4064556062221527\n",
      "Image 30 min: -0.7597121596336365, max: 0.9565672874450684\n",
      "Image 31 min: -0.05690986290574074, max: 0.24566754698753357\n",
      "Image 32 min: -0.8249661922454834, max: 0.9766919612884521\n",
      "Image 33 min: -0.27608156204223633, max: 0.6127656102180481\n",
      "Image 34 min: -0.21896813809871674, max: 0.5166986584663391\n",
      "Image 35 min: -0.28938403725624084, max: 0.6147178411483765\n",
      "Image 36 min: -0.05672542378306389, max: 0.2098960131406784\n",
      "Image 37 min: -0.4777376353740692, max: 0.7996483445167542\n",
      "Image 38 min: -0.33461329340934753, max: 0.674895703792572\n",
      "Image 39 min: -0.5826947093009949, max: 0.8520357608795166\n",
      "Image 40 min: -0.0541183166205883, max: 0.20758098363876343\n",
      "Image 41 min: -0.061540838330984116, max: 0.25991079211235046\n",
      "Image 42 min: -0.06414644420146942, max: 0.27864328026771545\n",
      "Image 43 min: -0.05669388547539711, max: 0.23098991811275482\n",
      "Image 44 min: -0.6147792935371399, max: 0.8835146427154541\n",
      "Image 45 min: -0.550294041633606, max: 0.8231505751609802\n",
      "Image 46 min: -0.3315180540084839, max: 0.6736210584640503\n",
      "Image 47 min: -0.32422447204589844, max: 0.6263457536697388\n",
      "Image 48 min: -0.7743957042694092, max: 0.9625644683837891\n",
      "Image 49 min: -0.04851064458489418, max: 0.16639679670333862\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate images\n",
    "print(imgs_fake[j].shape)\n",
    "# Generate images\n",
    "noise = Variable(Tensor(5*10, opt.latent_dim).normal_(0, 1))\n",
    "labels_onehot = Variable(Tensor(5*10, opt.n_classes).zero_())\n",
    "labels_ = torch.range(0, 9)\n",
    "labels_ = labels_.view(1, -1).repeat(5, 1).transpose(0, 1).contiguous().view(1, -1)\n",
    "labels_ = labels_.type(LongTensor)\n",
    "labels_ = labels_.view(-1, 1)\n",
    "labels_onehot = labels_onehot.scatter_(1, labels_, 1)\n",
    "imgs_fake = generator(noise, labels_onehot)\n",
    "\n",
    "# Save the generated images with labels overlaid\n",
    "save_dir = 'C:/Users/Timoy/Documents/GitHub/evml-evd3-project/Project_2/prep/prepgenerated_images'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Convert each generated image to PIL and overlay the label\n",
    "for j in range(imgs_fake.size(0)):\n",
    "    img = imgs_fake[j].cpu().detach().numpy().transpose(1, 2, 0)  # Convert tensor to numpy (H, W, C)\n",
    "\n",
    "    # Check the image range before applying any transformations\n",
    "    print(f\"Image {j} min: {img.min()}, max: {img.max()}\")  # Ensure the range is [-1, 1]\n",
    "\n",
    "    # Denormalize the image (convert from [-1, 1] to [0, 1])\n",
    "    img = (img + 1) / 2\n",
    "    \n",
    "    # Ensure the pixel values are in the range [0, 255]\n",
    "    img = (img * 255).clip(0, 255).astype('uint8')  # Convert to uint8 format for PIL\n",
    "    \n",
    "    # Squeeze the channel dimension (1, H, W) -> (H, W) for grayscale images\n",
    "    img = img.squeeze(axis=2)  # Remove the single channel dimension for grayscale images\n",
    "    \n",
    "    # Convert numpy array to PIL image\n",
    "    pil_img = Image.fromarray(img)\n",
    "    \n",
    "    # Overlay text (label) on the image\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    label = int(labels_[j].item())  # Get the label for the image\n",
    "    draw.text((10, 10), f'Label: {label}', fill='white')  # Draw text on image (top-left corner)\n",
    "    \n",
    "    # Save the image\n",
    "    pil_img.save(os.path.join(save_dir, f'epoch_{epoch}_batch_{i}_img_{j}_label_{label}.png'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MINOR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
